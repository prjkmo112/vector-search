import os
import sys
import json
import time
from typing import Any

import streamlit as st
import pandas as pd
from dotenv import load_dotenv
from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer
from fastembed import SparseTextEmbedding
from FlagEmbedding import BGEM3FlagModel

from imgtool import ImageWD
from PIL import Image
import io

from embedder import QPointBuilder, DenseOptionDataTypeEnum
from embedder.qtypes import DataItem


# Defaults from your repo
DEFAULT_COLLECTION_NAME = "commerce_product"
TEXT_EMBEDDING_MODEL = "Qdrant/bm25"
CLIP_MODEL_NAME = "clip-ViT-B-32"
BGEM3_MODEL_NAME = "BAAI/bge-m3"


load_dotenv()


def get_qdrant_client(host: str) -> QdrantClient:
    return QdrantClient(host)


@st.cache_resource(show_spinner=False)
def get_models(clip_model_name: str, bgem3_model_name: str, sparse_model_name: str):
    """Load all three models: CLIP (image), BGE-M3 (text), BM25 (sparse)"""
    clip = SentenceTransformer(clip_model_name)
    bgem3 = BGEM3FlagModel(bgem3_model_name, use_fp16=True)
    sparse = SparseTextEmbedding(model_name=sparse_model_name)
    return clip, bgem3, sparse


def build_sparse_vector(sparse_model: SparseTextEmbedding, text: str) -> models.SparseVector:
    emb = next(sparse_model.embed([text]))
    return models.SparseVector(indices=list(emb.indices), values=list(emb.values))


def build_dense_vector_from_image(clip_model: SentenceTransformer, image_bytes: bytes) -> list[float]:
    img = Image.open(io.BytesIO(image_bytes)).convert("RGB")
    vec = clip_model.encode(img)
    return vec.tolist()


def build_dense_vector_from_path_or_url(clip_model: SentenceTransformer, path_or_url: str) -> list[float]:
    img = ImageWD.open(path_or_url).convert("RGB")
    vec = clip_model.encode(img)
    return vec.tolist()


def build_dense_vector_from_text(clip_model: SentenceTransformer, text: str) -> list[float]:
    vec = clip_model.encode(text)
    return vec.tolist()


def pretty_json(obj: Any) -> str:
    try:
        return json.dumps(obj, ensure_ascii=False, indent=2)
    except Exception:
        return str(obj)


st.set_page_config(page_title="Vector Search UI (Qdrant)", layout="wide")

st.title("Qdrant ë²¡í„° ê²€ìƒ‰/ì ì¬ UI (Streamlit)")
st.caption("ì´ë¯¸ì§€(CLIP) + í…ìŠ¤íŠ¸(BGE-M3) dense ë²¡í„°, BM25 sparse ë²¡í„° ê²€ìƒ‰")

with st.sidebar:
    st.header("ì—°ê²° ì„¤ì •")
    qdrant_host = st.text_input("Qdrant Host", value=os.getenv("QDRANT_CLIENT_IP", "http://localhost:6333"))
    collection_name = st.text_input("Collection", value=os.getenv("QDRANT_COLLECTION", DEFAULT_COLLECTION_NAME))

    st.divider()
    st.subheader("ëª¨ë¸ ì„¤ì •")
    clip_model_name = st.text_input("CLIP ëª¨ë¸ (ì´ë¯¸ì§€)", value=os.getenv("CLIP_MODEL", CLIP_MODEL_NAME))
    bgem3_model_name = st.text_input("BGE-M3 ëª¨ë¸ (í…ìŠ¤íŠ¸)", value=os.getenv("BGEM3_MODEL", BGEM3_MODEL_NAME))
    sparse_model_name = st.text_input("Sparse ëª¨ë¸ (BM25)", value=os.getenv("TEXT_EMBEDDING_MODEL", TEXT_EMBEDDING_MODEL))

    st.divider()
    st.subheader("í‘œì‹œ")
    limit = st.slider("ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜", 1, 100, 20)
    show_payload = st.checkbox("payload í‘œì‹œ", value=True)

clip_model, bgem3_model, sparse_text_model = get_models(clip_model_name, bgem3_model_name, sparse_model_name)
qclient = get_qdrant_client(qdrant_host)

tabs = st.tabs(["ğŸ” ê²€ìƒ‰", "â„¹ï¸ ì»¬ë ‰ì…˜/í—¬ìŠ¤"])

# -------------------------
# Search tab
# -------------------------
with tabs[0]:
    st.subheader("ê²€ìƒ‰")

    colA, colB = st.columns([1.2, 1])
    with colA:
        algo = st.radio("ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜", ["dense_image", "dense_text", "sparse", "hybrid"], horizontal=True, index=0)

        # sparse/hybridì—ì„œë§Œ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ í‘œì‹œ
        query_text = ""
        if algo in ("sparse", "hybrid", "dense_text"):
            query_text = st.text_input("í…ìŠ¤íŠ¸ ì¿¼ë¦¬", value="ë£¨ì´ë³´ìŠ¤ì°¨")

        # dense_image/hybridì—ì„œë§Œ ì´ë¯¸ì§€ ì…ë ¥ í‘œì‹œ
        uploaded = None
        dense_path_or_url = ""
        if algo in ("dense_image", "hybrid"):
            dense_source = st.selectbox("ì´ë¯¸ì§€ ì…ë ¥", ["ì—…ë¡œë“œ ì´ë¯¸ì§€", "ì´ë¯¸ì§€ URL/ê²½ë¡œ"], index=0)
            if dense_source == "ì—…ë¡œë“œ ì´ë¯¸ì§€":
                uploaded = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["png", "jpg", "jpeg", "webp"])
            else:
                dense_path_or_url = st.text_input("ì´ë¯¸ì§€ URL ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ", value="")
        else:
            dense_source = None

        run = st.button("ê²€ìƒ‰ ì‹¤í–‰", type="primary")

    with colB:
        st.markdown("#### íŒ")
        st.markdown(
            """
- **dense_image**: CLIP ì´ë¯¸ì§€ ë²¡í„°ë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰ (image_dense)
- **dense_text**: BGE-M3 í…ìŠ¤íŠ¸ ë²¡í„°ë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰ (text_property_dense)
- **sparse**: BM25 ê¸°ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰ (text_title_sparse)
- **hybrid**: sparseë¡œ 1ì°¨ í›„ë³´ â†’ image denseë¡œ ì¬ì •ë ¬
            """.strip()
        )

    if run:
        # Validate inputs based on algorithm
        if algo in ("sparse", "hybrid") and not query_text.strip():
            st.error("sparse/hybrid ê²€ìƒ‰ì€ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            st.stop()

        if algo == "dense_text" and not query_text.strip():
            st.error("dense_text ê²€ìƒ‰ì€ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            st.stop()

        # Build image dense vector if needed
        image_dense_vector = None
        if algo in ("dense_image", "hybrid"):
            try:
                if dense_source == "ì—…ë¡œë“œ ì´ë¯¸ì§€":
                    if uploaded is None:
                        st.error("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
                        st.stop()
                    image_bytes = uploaded.getvalue()
                    st.image(image_bytes, caption="Query Image", use_container_width=True)
                    img = Image.open(io.BytesIO(image_bytes)).convert("RGB")
                    image_dense_vector = clip_model.encode(img).tolist()
                else:  # ì´ë¯¸ì§€ URL/ê²½ë¡œ
                    if not dense_path_or_url.strip():
                        st.error("ì´ë¯¸ì§€ URL/ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                        st.stop()
                    image_dense_vector = build_dense_vector_from_path_or_url(clip_model, dense_path_or_url.strip())
                    try:
                        st.image(dense_path_or_url.strip(), caption="Query Image", use_container_width=True)
                    except Exception:
                        pass
            except Exception as e:
                st.exception(e)
                st.stop()

        # Build text dense vector if needed
        text_dense_vector = None
        if algo == "dense_text":
            try:
                # BGE-M3 returns dict with 'dense_vecs'
                result_dict = bgem3_model.encode([query_text])
                if 'dense_vecs' in result_dict:
                    text_dense_vector = result_dict['dense_vecs'][0].tolist()
                else:
                    st.error("BGE-M3 ëª¨ë¸ ì¶œë ¥ í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤.")
                    st.stop()
            except Exception as e:
                st.exception(e)
                st.stop()

        try:
            with st.spinner("Qdrant ì¡°íšŒ ì¤‘..."):
                if algo == "dense_image":
                    result = qclient.query_points(
                        collection_name=collection_name,
                        query=image_dense_vector,
                        using="image_dense",
                        limit=limit,
                        with_payload=True
                    )
                elif algo == "dense_text":
                    result = qclient.query_points(
                        collection_name=collection_name,
                        query=text_dense_vector,
                        using="text_property_dense",
                        limit=limit,
                        with_payload=True
                    )
                elif algo == "sparse":
                    sparse_vec = build_sparse_vector(sparse_text_model, query_text)
                    result = qclient.query_points(
                        collection_name=collection_name,
                        query=sparse_vec,
                        using="text_title_sparse",
                        limit=limit,
                        with_payload=True
                    )
                else:  # hybrid
                    sparse_vec = build_sparse_vector(sparse_text_model, query_text)
                    result = qclient.query_points(
                        collection_name=collection_name,
                        prefetch=[
                            models.Prefetch(
                                query=sparse_vec,
                                using="text_title_sparse",
                                limit=max(limit * 5, 50),
                            )
                        ],
                        query=image_dense_vector,
                        using="image_dense",
                        limit=limit,
                        with_payload=True
                    )
        except Exception as e:
            st.exception(e)
            st.stop()

        points = getattr(result, "points", []) or []
        if not points:
            st.warning("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            rows = []
            for p in points:
                payload = p.payload or {}
                rows.append({
                    "score": float(p.score) if p.score is not None else None,
                    "id": p.id,
                    "product_id": payload.get("product_id"),
                    "title": payload.get("title"),
                    "url": payload.get("url") or payload.get("image_url") or payload.get("image"),
                    "payload": payload if show_payload else None,
                })

            df = pd.DataFrame(rows)
            st.dataframe(df.drop(columns=["payload"]) if not show_payload else df, use_container_width=True, height=360)

            st.markdown("#### ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸°")
            cols = st.columns(5)
            for i, r in enumerate(rows[:25]):
                url = r.get("url")
                if not url:
                    continue
                with cols[i % 5]:
                    try:
                        st.image(url, caption=f"{r.get('score'):.4f} / {r.get('product_id')}", use_container_width=True)
                    except Exception:
                        st.caption("ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")
                        st.text(url)

            if show_payload:
                st.markdown("#### payload ìƒì„¸")
                for r in rows[:min(10, len(rows))]:
                    with st.expander(f"{r.get('product_id')} Â· {r.get('title')} Â· score={r.get('score'):.5f}"):
                        st.code(pretty_json(r["payload"]), language="json")

# -------------------------
# Info tab
# -------------------------
with tabs[1]:
    st.subheader("Qdrant ìƒíƒœ/ì»¬ë ‰ì…˜ ì •ë³´")
    col1, col2 = st.columns(2)
    with col1:
        if st.button("í—¬ìŠ¤ ì²´í¬"):
            try:
                st.json(qclient.get_collections().model_dump() if hasattr(qclient.get_collections(), "model_dump") else qclient.get_collections())
            except Exception as e:
                st.exception(e)
    with col2:
        if st.button("ì»¬ë ‰ì…˜ ìƒì„¸"):
            try:
                info = qclient.get_collection(collection_name)
                st.json(info.model_dump() if hasattr(info, "model_dump") else info)
            except Exception as e:
                st.exception(e)
